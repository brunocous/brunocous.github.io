<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bruno  Coussement | blog - page 2</title>
    <meta name="author" content="Bruno  Coussement" />
    <meta name="description" content="Freelance machine learning and data engineer
" />
    <meta name="keywords" content="ML, data, engineering, CB" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🔥</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://brunocous.github.io/blog/page/2/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://brunocous.github.io/"><span class="font-weight-bold">Bruno</span>   Coussement</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <div class = "toggle-container">
                <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <div class="post">

  <div class="header-bar">
    <h1>bruno's blog</h1>
    <h2>Technical articles</h2>
  </div>


  <ul class="post-list">
    
    
    


    

    <li>
      <h3><a class="post-title" href="https://medium.com/datamindedbe/what-i-wish-i-knew-before-going-into-data-engineering-ddb659b4a05?source=rss-6c50ab7de1d9------2">What I wish I knew before going into Data Engineering</a>
      </h3>
      <!-- <p><p><strong>Disclaimer</strong>: this is my opinion, not necessarily the one of my employer or any organisation.</p><p>There is a clear difference between what I expected of the job, vs what I know now after 2.5 years on the job. Maybe not every item stated below applies to you, but some might.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IE38VsVaVb82Cfg3" /><figcaption>Photo by <a href="https://unsplash.com/@supernov?utm_source=medium&amp;utm_medium=referral">Edi Libedinsky</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><h3>My background</h3><p>This might help you understand the points stated below.</p><p>I completed my Master in Mathematical Engineering at the KU Leuven (Belgium) focussing on high-perfomance computing and machine learning. As most Belgians obtaining a uni degree at 23–24 years of age, I start working right away, without taking a break to discover the world or something like that.</p><p>I worked 5–6 months as a data analyst creating excel reports, but missing the technical element that I studied for. I swapped the industry for academia, starting a PhD in machine learning. I got to work on a super cool topic, but was alone on my island.</p><p>So data engineering, which happens mostly in teams, yet quite technical, hits the sweet spot for me.</p><h3>So, what should I know?</h3><p>I present each statement as ignorant me <strong>👼</strong>, and the still ignorant but older me <strong>👴</strong>.</p><h4><strong>👼 Data scientists have the sexiest job of the 21st century<br>👴 Data engineers are currently in higher demand</strong></h4><p>The <a href="https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century">2012 infamous article about data science being the sexiest job of the 21 century</a> surely gave the field of data science, machine learning and AI a lot of attention it rightfully deserves. The thing is, to deliver a successful data science (or ML or AI if you prefer to hear buzzwords) project, it takes a lot more than few domain experts answering a high-priority business question, and able to run model.train() and model.predict() optimising a certain metric or KPI. This message is nicely conveyed in <a href="https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007">the AI hierarchy of needs</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*27SxmduKF_R-1ssp2Q_d_A.png" /><figcaption>Slide by author</figcaption></figure><p>In a nutshell, to do the impactful things at the top well, you need a good base layers. That’s what most (data) engineers do. This is reflected in the number of search hits on the career search tool of for example AWS: 24k for data engineers, 8k for machine learning, 18k for data science.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*R9djOlVU7OxpTBGrepwlnQ.png" /><figcaption>Number of job opportunities at AWS. Search performed on 25th of March 2021. Screenshots by author</figcaption></figure><p>A similar result can be found when searching on LinkedIn: more hits for data engineers than data scientist. I also included the number of hits based on “marketing”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*a3GhPeACwqFio81I4K-u6A.png" /><figcaption>Number of hits per search term on LinkedIn. Search performed on 25th of March 2021. I have no affiliations with any of the companies in the results. Screenshots by author</figcaption></figure><p>I also believe it will also be a good time to go into data engineering, because solid base layers (of the pyramid above) will always be needed.</p><h4><strong>👼 I need to know ALL of the technologies: bottom-up <br>👴 Take a top-down approach</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hEggrWe4AhklWnrNsDg0Iw.png" /><figcaption>Screenshots by author</figcaption></figure><p>When going through data engineering job requirements, it is easy to think that you need to master all of them. That’s not true, it is a recruiter’s wet dream.</p><p>The technical landscape is vast, and too much for a single person to fully grasp all of its intricate details.</p><p>Why? Technologies evolve all the time. The world evolve all the time. Being able to evaluate the fitness of a certain technology on a given problem and context to get the job done fast or sustainably is far more important.</p><p>Sure, having deep knowledge in a few of them will help you, but just knowing where to look or who to ask is often fine.</p><p>Important here is that you get excited by learning new tools/technologies A LOT, emphasising A LOT.</p><h4><strong>👼 I will need to build my own data engineering tools<br>👴 Re-using a tool that is not perfect fits 99% of problems and situations</strong></h4><p>Each problem, situation you encounter as a data engineer is unique. I used to think that often I will need to write or build a tool specifically for it.</p><p>If you work for the Ubers, Facebooks and Googles of the world, with huge engineering capabilities, then yes, you might contribute to it. For the others, existing tooling is fine for 99% of the time.</p><p>For the 1% niche problems and situations, they are probably low-priority anyway. If you work for a business unit still wanting you to find a solution to it, then your best shot is adapting an existing opensource tool close to it. Don’t forget to contribute it back to the community. At least it doesn’t die after you left for another organisation.</p><h4><strong>👼 Every company has about the same definition of what data engineering is<br>👴 Each has its own definition</strong></h4><p>Data engineers come in many forms, responsibilities and degress of business involvement. Here a three common types I encountered:</p><ul><li><strong>Jack of all trades</strong></li></ul><p>They build and manage the whole data platform, meanwhile building and deploying several ML models, meanwhile owning and managing the project, meanwhile being the person translating business requirements in technical ones.</p><p>This type is commonly found in young startups doing X or Y “with machine learning at scale”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x9HzMwTjG1JSXPk4" /><figcaption>Photo by <a href="https://unsplash.com/@standsome?utm_source=medium&amp;utm_medium=referral">Standsome Worklifestyle</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><ul><li><strong>The re-branded database or system admin</strong></li></ul><p>Only maintains or offers support for the data platform. They don’t like to talk to business. Mostly reachable through IT ticketing systems.</p><p>Don’t get me wrong, they will know every single obscure detail of the solution they manage, going full vertical. Just don’t expect too much horizontal movement from them.</p><p>You can find them in large multi-national organisations. Having these profiles at that scale can make sense.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ASuVza9ScafVTsFH" /><figcaption>Photo by <a href="https://unsplash.com/@superadmins?utm_source=medium&amp;utm_medium=referral">Sammyayot254 @ https://superadmins.co</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><ul><li><strong>The model make-up artist</strong></li></ul><p>While data engineering is broad, they are mostly making sure ML models are ready to go into production. They might or might not be embedded in business units. Little or no data platform administration.</p><p>You can find them in more mature ML organisations with &gt;50 ML models running in production.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CiJb8hMzPbsvue5r" /><figcaption>Photo by <a href="https://unsplash.com/@frame_media?utm_source=medium&amp;utm_medium=referral">René Ranisch</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Jokes aside, just talk to the actual data engineering teams to get an actual feel of what they do.</p><h4><strong>👼 The biggest challenges for delivering a successful project on time are technical<br>👴 Most challenges are related to the company’s own culture, processes</strong></h4><p>Ok, forget all the previous ones. If I want you to remember one, then this one it is.</p><p>Projects and people can change, typical data/IT/expectation challenges remain. The ones that takes most of your time and sweat are always rooted in the organisation’s culture.</p><p>No need to lecture you on how hard a culture change is. But it starts with you just naming and escalating those challenges consistently.</p><p>If you manage to solve or at least improve upon one of them, Congrats! The data world needs more heroes like you.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FtmFmS14tRBJtnm3j1fqXw.png" /><figcaption>Slides from author. Resemblance to a real situation is purely random.</figcaption></figure><h3>Go for data engineering if</h3><ol><li>You love to think like an engineer = in terms of systems, stability, reliability, fault tolerance, efficiency, automation everywhere,</li><li>You recognise that data science is 80% data engineering and want to name it properly 😉,</li><li>You love to learn a new piece of technology with every new project,</li><li>You are a true team player: <br>→ you get energy from good discussions, <br>→ achieving something as a group,<br>→ love to learn from more experienced members.</li></ol><h3><strong>As a complete novice wanting to go into data engineering, you can prepare by minimally</strong></h3><p>✅ Having deep understanding or demonstrated experience in a programming language (preferably Python),</p><p>✅ Sharpening your communication skills,</p><p>✅ Having played around with a machine learning in classes or on your own.</p><p>The rest will come on the job!</p><h3>Impress future employers by</h3><ul><li>Obtaining a cloud certification (AWS, GCP or Azure),</li><li>Demonstrating experience with building and running Docker containers,</li><li>Learning about DataOps, MLOps, DevSecOps,</li><li>Playing with Kubernetes (or any container orchestration framework),</li><li>Learning Terraform or other infrastructure-as-code framework,</li><li>Playing with data engineering specific components: workflow orchestrators (Airflow), data quality tools,</li><li>Demonstrating your experience in a parallel computing framework (Dask, Pyspark, OpenMPI, Horovod, etc),</li><li>Contributing to an open-source project on Github,</li><li>Experience in change management.</li></ul><h3>Conclusion</h3><p>As Andrew Ng’s says (according to many <a href="https://www.instagram.com/ml__memes/">memes</a>): Don’t worry about it if you don’t understand it. You’ll see.</p><p>Interested? <a href="https://www.dataminded.be/">Data minded</a> is always looking for talented (future) data engineers!</p><h4>Acknowledgements</h4><p><em>Thank you Data Minded for all the learning opportunities in order to publish this post.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ddb659b4a05" width="1" height="1" alt=""><hr><p><a href="https://medium.com/datamindedbe/what-i-wish-i-knew-before-going-into-data-engineering-ddb659b4a05">What I wish I knew before going into Data Engineering</a> was originally published in <a href="https://medium.com/datamindedbe">datamindedbe</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p> -->
      <p class="post-meta"> 8 min read &nbsp; &middot; &nbsp;
        June 9, 2021
      </p>
      <p class="post-tags">
        <a href="2021">
          <i class="fas fa-calendar fa-sm"></i> 2021 </a>

          
          &nbsp; &middot; &nbsp;
            
            <a href="what-i-wish-i-knew">
              <i class="fas fa-tag fa-sm"></i> what-i-wish-i-knew</a> &nbsp;
              
            <a href="how-to">
              <i class="fas fa-tag fa-sm"></i> how-to</a> &nbsp;
              
            <a href="what-i-wish-id-known">
              <i class="fas fa-tag fa-sm"></i> what-i-wish-id-known</a> &nbsp;
              
            <a href="data-engineering">
              <i class="fas fa-tag fa-sm"></i> data-engineering</a> &nbsp;
              
          
    </p>
    </li>

    
    
    


    

    <li>
      <h3><a class="post-title" href="https://medium.com/datamindedbe/ml-pipelines-in-azure-machine-learning-studio-the-right-way-26b1f79db3f8?source=rss-6c50ab7de1d9------2">ML Pipelines in Azure Machine Learning Studio the right way</a>
      </h3>
      <!-- <p><h3>ML Pipelines in Azure Machine Learning the right way</h3><h4>A code example to get you up-and-running quickly</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*86uBC9hpyES-Txfb" /><figcaption>Photo by <a href="https://unsplash.com/@allodium?utm_source=medium&amp;utm_medium=referral">Florian Wächter</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>The <a href="https://docs.microsoft.com/en-us/azure/machine-learning/">official Azure Machine Learning Studio documentation</a>, the <a href="https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py">Python SDK reference</a> and the <a href="https://github.com/Azure/MachineLearningNotebooks">notebook examples</a> <strong>are often out-of-date</strong>, or <strong>don’t cover all important aspects, </strong>or <strong>don’t provide a compelling end-to-end example</strong>. This guide is an attempt to cover the necessary basics, hopefully accelerating you in building a machine learning pipeline on Azure.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*jp2Q4PTmFPCek02g" /><figcaption>Azure ML Studio logo</figcaption></figure><h3>Azure ML Studio</h3><p>Azure ML Studio (AML) is an Azure service for data scientists to build, train and deploy models. Data engineers on the other hand can use it as a starting point to industrialise ML models.</p><p>According to the MLOps best-practices from <a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">Google</a> or <a href="https://azure.microsoft.com/en-us/services/machine-learning/mlops/">Microsoft</a>, you actually want to build a pipeline of defined steps (data preparation, hyper-parameter tuning, model training, model evaluation) instead of merely developing “a model”. Although this approach requires more effort than adhering to an ad-hoc notebook flow, some clear benefits make up for it:</p><ul><li>enables rapid structured end-to-end experimentation,</li><li>separation of concerns,</li><li>easier to share a single or multiple component across projects,</li><li>more possibilities for automation.</li></ul><h3>Getting started</h3><p>Before diving into code and configurations, make sure you have checked-off the pre-requisites. This article will not cover it, because the links should enable you to do it.</p><h4>Pre-requisites</h4><ul><li>Have an <a href="https://azure.microsoft.com/en-us/free/search/">account on Azure</a>.</li><li>Created an <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python">Azure ML Workspace</a>.</li><li>Created a basic <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python">Azure ML Compute cluster</a>.<br>Three nodes of a cheap instance type (e.g. Standard_DS2_v2) is more than enough for this tutorial.</li><li>Created an <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data">Azure ML Datastore</a> for the resource your data resides on (typically a Container in a Storage Account).</li><li>Ran pip install azureml-core azureml-pipelinein your development environment</li></ul><p>We are going to build a two step pipeline:</p><ol><li>data preparation step,</li><li>model training step.</li></ol><p>This is to keep it simple, but that you can at least understand how to feed the output of a step into another.</p><p>All the code can be found <a href="https://github.com/brunocous/azureml-pipeline-demo">here</a>. In order to make it run, you will need to add your own workspace, datastore and compute config.</p><h4>#1. Create a reference to the Azure ML Workspace</h4><p>The Workspace is the fundamental Azure ML resource. It is tied to a subscription and resource group. You would typically have a single workspace per project.</p><pre>from azureml.core import Workspace</pre><pre>ws = Workspace.get(<br>           name=&quot;my_workspace&quot;,<br>           subscription_id= &quot;111&quot;,<br>           resource_group= &quot;my_resource_group&quot;<br>)</pre><p>Note, that you can also load configurations<a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py#from-config-path-none--auth-none---logger-none---file-name-none-"> from a file</a> using Workspace.from_config(). Save it in &lt;project-root&gt;/.azureml/config.json.</p><h4>#2. Create a reference to the Azure ML Datastore</h4><p>A datastore is an AML-specific component that abstracts away the Azure resource the data is stored on. It allows for cross-subscription or cross-resource group data access on Azure (useful in enterprise context). The main advantage is that takes care of authentication for you (after initial setup, minimal key management is required).</p><p>You don’t necessarily need it. If you don’t, be ready to pollute your code with authentication/authorization snippets. More code = more liability!</p><pre>from azureml.core.datastore import Datastore</pre><pre>datastore = Datastore.get(<br>    workspace=ws, <br>    datastore_name=&quot;my_datastore&quot;<br>)</pre><h4>#3. Register input files saved on blob store as an Azure ML Dataset</h4><p>Azure ML Datasets allow you to for example register files in a blob store as a File (images, texts, sound recordings,…) or Tabular (parquet, csv, json,…) dataset. It allows you to version, monitor, profile and quick-preview it.</p><p>I do recommend to use it, even though you don’t absolutely need it. You could interact to Blob store directly from your pipeline step. The con is that you’ll miss some built-in features (dataset monitor for data drift, etc). In the end, Azure ML Datasets basically is a pointer to your files, and keeps some metadata on it.</p><p><strong>Tip</strong>: If registering large tables (&gt;100M rows and/or &gt;100 cols) from partitioned parquet files as Tabular Dataset gives troubles, then use File Dataset type instead. The latter type doesn’t perform a data validation step upon creation.</p><p>Registering input data as a AML Dataset happens typically as the last step of an ingestion process. For simplicity, I assume you have a Pandas DataFrame that you would like to register as a Tabular Dataset called “my_input_dataset”.</p><p>The basic pattern is to save it locally, upload the files to a datastore and register is.</p><pre>from azureml.core import Dataset</pre><pre>df = _ # assumption: custom code here to obtain df</pre><pre>local_path = &quot;path/to/local/parquet/files&quot;<br>target_path = &quot;path/to/upload/on/blob&quot;</pre><pre>df.to_parquet(local_path, index=False)</pre><pre>data_reference = datastore.upload(src_dir=local_path, target_path=target_path, overwrite=True)</pre><pre>dataset = Dataset.Tabular.from_parquet_files(path=data_reference)</pre><pre>dataset.register(workspace=ws, name=&quot;my_raw_dataset&quot;)</pre><p>Side note, at the time of writing, I discovered <a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?preserve-view=true&amp;view=azure-ml-py#methods">a function in preview</a> which avoids you to manually save, upload and parse your dataframe.</p><pre>Dataset.Tabular.register_pandas_dataframe(dataframe=df, target=target_path, name=&quot;my_raw_dataset&quot;)</pre><h4>#4. Create Python modules, but separate Azure ML specific code from the rest</h4><p>Adding all AML specific code to an azureml folder avoids polluting your non-cloud-vendor code. This renders the general and the azure-specific part more fit for re-use.</p><pre>├── src/<br>    └── my_awesome_project<br>        ├── data<br>        │   └── clean_input_data.py<br>        ├── ml<br>        │   └── model.py<br>        └── azureml<br>            ├── aml_clean_data.py<br>            ├── aml_train_model.py<br>            ├── register_dataframe_as_dataset.py<br>            └── create_and_trigger_pipeline.py</pre><p>The aml_* Python scripts will be executed by the AML compute cluster. The other two, serve as scripts to trigger remote execution from your local laptop.</p><p>The remotely executed script aml_clean_data.py reads an input Azure ML dataset, cleans it, and saves it to a mounted path. Registering the output data happens automatically if the script described here ran successfully (see step 5).</p><p>A Run represents <a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py">a single trial of and experiment</a>. Runs are used to monitor the asynchronous execution of a trial, log metrics and store output of the trial, and to analyze results and access artifacts generated by the trial.</p><pre>import os<br>from azureml.core import Run<br>from my_awesome_project.data.clean_input_data import clean</pre><pre>run = Run.get_context()<br>raw_df = run.input_datasets[&quot;my_raw_dataset&quot;].to_pandas_dataframe()<br><br>clean_df = clean(raw_df)</pre><pre>mounted_output_dir = run.output_datasets[&quot;my_clean_dataset&quot;]</pre><pre>os.makedirs(os.path.dirname(mounted_output_dir), exist_ok=True)<br>clean_df.to_parquet(mounted_output_dir)</pre><p>The other remotely executed aml_train_model.py reads a cleaned dataset, trains a model, and registers it. Note, that everything in /outputs and /logs will be uploaded to the run. Behind the scenes, the trained model will be saved on a blob store managed by Azure ML.</p><p>To pass model parameters, an easy option is to use an ArgumentParser to parse command line arguments that will be set by Azure ML when running this script.</p><pre>from argparse import ArgumentParser<br>from azureml.core import Run<br>from my_awesome_project.ml.model import train_model</pre><pre>run = Run.get_context()<br>ap = ArgumentParser()<br>ap.add_argument(&quot;--epochs&quot;)<br>args = ap.parse_args()</pre><pre><br>clean_df = run.input_datasets[&quot;my_clean_dataset&quot;].to_pandas_dataframe()</pre><pre>trained_model = train_model(data=clean_df, epochs=args.epochs)</pre><pre>trained_model.save(&quot;./outputs/model&quot;) # /outputs is important</pre><pre>run.register_model(name=&quot;my_model&quot;, path=&quot;outputs/model&quot;)</pre><h4>#5. Define inputs as dataset named-references, and outputs as OutputFileDatasetConfig</h4><p>This is the most confusing part of getting started if you go through notebook examples. You will find so many classes that basically do the same. Their class-structure feels messy and unpythonic. To convince yourself of that, take a look at all the possible input types a PythonScriptStep can accept.</p><blockquote>&lt;azureml.pipeline.core.graph.InputPortBinding,<br>azureml.data.data_reference.DataReference,<br>azureml.pipeline.core.PortDataReference,<br>azureml.pipeline.core.builder.PipelineData,<br>azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset,<br>azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset,<br>azureml.data.dataset_consumption_config.DatasetConsumptionConfig&gt;</blockquote><p>Or the output types:</p><blockquote>&lt;azureml.pipeline.core.builder.PipelineData,<br>azureml.data.output_dataset_config.OutputDatasetConfig,<br>azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset,<br>azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset,<br>azureml.pipeline.core.graph.OutputPortBinding&gt;</blockquote><p>Indeed, go figure it out 😉.</p><p>An easy way is to use Dataset.get_by_name to create an input data reference. Output or intermediary data is best reference by creating an OutputFileDatasetConfig object.</p><p>For example, reference the raw and clean dataset in create_and_trigger_pipeline.py are done as follows:</p><pre>from azureml.core import Dataset<br>from azureml.data.output_dataset_config import OutputFileDatasetConfig</pre><pre>input_data = Dataset.get_by_name(workspace=ws, name=&quot;my_raw_data&quot;)</pre><pre>clean_data = (<br>    OutputFileDatasetConfig(<br>        name=&quot;my_clean_data&quot;, <br>        destination=(<br>            datastore,<br>            &quot;path/on/blob/to/write/clean/data/to&quot;)<br>    ).as_upload(overwrite=True)<br>    .read_parquet_files()  # To promote File to Tabular Dataset<br>    .register_on_complete(name=&quot;my_clean_data&quot;)<br>)</pre><p>Note, if you are working with tabular data, and you wish to work with Tabular datasets from the output, then you need to convert the default File Dataset to a Tabular one. This is done by calling read_parquet_files() on the OutputFileDatasetConfig .</p><h4>#6. Create and configure PythonScriptStep</h4><p>In the previous step we defined a script that we would like to run on a compute platform managed by Azure ML. Again, there are <a href="https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py">multiple options here</a>. For all Python scenarios, I recommend using the PythonScriptStep . Except if you need <a href="https://spark.apache.org/docs/latest/api/python/">pyspark</a>, then consider using the DatabricksStep .</p><p>For the runconfig parameter, look <a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.runconfiguration?view=azure-ml-py">at the documentation</a> of RunConfiguration in order to set dependencies (and others) correctly for your project.</p><p>An example of step definitions in create_and_trigger_pipeline.py could look like:</p><pre>from azureml.core.compute import ComputeTarget<br>from azureml.core.runconfig import RunConfiguration<br>from azureml.pipeline.core.pipeline import Pipeline<br>from azureml.pipeline.steps import PythonScriptStep<br>from pathlib import Path<br>from my_awesome_project.azureml import aml_clean_data, aml_train_model<br>import my_awesome_project</pre><pre># dataset references here (see step 5)</pre><pre>src_dir = Path(my_awesome_project.__file__).parent.parent<br>clean_mdl_path = Path(aml_clean_data.__file__).relative_to(src_dir)<br>train_mdl_path = Path(aml_train_model.__file__).relative_to(src_dir)</pre><pre>clean_step = (<br>    PythonScriptStep(<br>        name=&quot;clean data&quot;, <br>        script_name=str(clean_mdl_path),<br>        source_directory=src_dir, <br>        runconfig=RunConfiguration(),<br>        inputs=[input_data<br>            .as_named_input(&quot;my_raw_data&quot;)<br>            .as_mount()],<br>        outputs=[clean_data],<br>        compute_target=ComputeTarget(<br>            workspace=ws, <br>            name=&quot;small_cluster&quot;),<br>        allow_reuse=True,<br>)</pre><pre>train_step = (<br>    PythonScriptStep(<br>        name=&quot;train_model&quot;, <br>        script_name=str(train_mdl_path),<br>        source_directory=src_dir, <br>        runconfig=RunConfiguration(),<br>        arguments=[&quot;--epochs&quot;, &quot;5&quot;],<br>        inputs=[clean_data.as_input()],<br>        outputs=[],<br>        compute_target=ComputeTarget(<br>            workspace=ws, <br>            name=&quot;small_cluster&quot;),<br>        allow_reuse=True,<br>)</pre><h4><strong>#7. Submit your pipeline to an Azure ML Experiment and trigger a run</strong></h4><p>Finally to submit your pipeline your pipeline to an experiment, and trigger it, simply.</p><p>To do this in create_and_trigger_pipeline.py , simply</p><pre>from azureml.core.experiment import Experiment<br>from azureml.pipeline.core.pipeline import Pipeline</pre><pre>exp = Experiment(workspace=ws, name=&quot;my_experiment&quot;)<br>pipeline = Pipeline(ws, steps=[clean_step, train_step])<br>run = pipeline.submit(experiment_name=exp.name)</pre><pre>run.wait_for_completion(raise_on_error=True)</pre><p>Visit your workspace through the Azure portal, and you should be seeing a running pipeline.</p><h3>Next steps? More steps!</h3><p>After getting a basic flow up-and-running, you should be able to add more steps in a similar way. Think about data validation, model validation, more data cleaning, etc.</p><p>Also think about abstracting away all the boilerplate code that I left here explicitly for educational purposes.</p><p><strong>Tip</strong>: Log any performance metric, tables, visualisations to the experiment run. For example, let’s say you’re predicting bicycle-rent demand, and business wants you to focus on a few renting points around train stations. In order to easily track progress on that, you could log the mean absolute error of the predictions made around those high-interest points.<br>Look at the <a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run%28class%29?preserve-view=true&amp;view=azure-ml-py#log-name--value--description----">documentation of Run</a> for reference.</p><pre>mae = mae_around_train_stations(model, X_test, y_test)</pre><pre>run.log(name=&quot;mae_around_stations&quot;, value=mae, description=&quot;MAE predictions around train stations&quot;)</pre><h4>Industrialisation</h4><p>Once you got a basic pipeline working, and you get consistent performance, it is time to think about industrialisation. For small use cases, it is perfectly fine to just promote this pipeline to a production context. For larger projects, think about how to integrate this flow with stable engineering flows. Avoid creating a pipeline jungle introducing yet another pipelining tool.</p><p>In order to increase stability, observability; think about the following:</p><ul><li>integrate with a central orchestrator of choice (Airflow, Azure Data Factory, etc),</li><li>setup a CI/CD flow,</li><li>setup data and model monitoring correctly.</li></ul><p>Azure obviously <a href="https://github.com/microsoft/MLOps">has an offering for this</a>. If your organisation has the “go full Azure managed services”-strategy, then go ahead. Else, substitute each mentioned service by a cloud-agnostic one (for example Azure Data Factory by Airflow). Do the math about the total cost of managing these solutions.</p><h3>Conclusion</h3><p>We proposed a basic example on how to use Azure ML pipelines in batch context. It avoids the use of the deprecated Azure ML Python SDK components.</p><h3>A proof-of-concept in your organisation?</h3><p>Contact Data Minded through <a href="https://www.linkedin.com/company/data-minded/mycompany/">https://www.linkedin.com/company/data-minded/</a> or <a href="https://www.dataminded.be/">https://www.dataminded.be/</a>.</p><h4>Acknowledgements</h4><p><em>I would like to thank Data Minded for having offered me learning opportunities at different clients.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=26b1f79db3f8" width="1" height="1" alt=""><hr><p><a href="https://medium.com/datamindedbe/ml-pipelines-in-azure-machine-learning-studio-the-right-way-26b1f79db3f8">ML Pipelines in Azure Machine Learning Studio the right way</a> was originally published in <a href="https://medium.com/datamindedbe">datamindedbe</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p> -->
      <p class="post-meta"> 10 min read &nbsp; &middot; &nbsp;
        June 7, 2021
      </p>
      <p class="post-tags">
        <a href="2021">
          <i class="fas fa-calendar fa-sm"></i> 2021 </a>

          
          &nbsp; &middot; &nbsp;
            
            <a href="batch">
              <i class="fas fa-tag fa-sm"></i> batch</a> &nbsp;
              
            <a href="python">
              <i class="fas fa-tag fa-sm"></i> python</a> &nbsp;
              
            <a href="azure-ml-studio">
              <i class="fas fa-tag fa-sm"></i> azure-ml-studio</a> &nbsp;
              
            <a href="pipeline">
              <i class="fas fa-tag fa-sm"></i> pipeline</a> &nbsp;
              
            <a href="azure-ml">
              <i class="fas fa-tag fa-sm"></i> azure-ml</a> &nbsp;
              
          
    </p>
    </li>

    
    
    


    

    <li>
      <h3><a class="post-title" href="https://medium.com/datamindedbe/what-to-consider-before-choosing-argo-workflow-54f6067307a8?source=rss-6c50ab7de1d9------2">What to consider before choosing Argo Workflow?</a>
      </h3>
      <!-- <p><h4>To go full Kubernetes-native or not?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*s-mLYZi_shXBKYORf8k2dQ.png" /><figcaption>Logo from <a href="https://argoproj.github.io/projects/argo">https://argoproj.github.io/</a></figcaption></figure><p>The <a href="https://huyenchip.com/2020/06/22/mlops.html">recent explosion of tools</a> including task and data orchestration tools should make you wonder if you’re still doing the right thing. Purely based on Github-stars of the open-source frameworks, Airflow is still the most popular one. This does not take into account the popularity of closed-source, or cloud vendor tools. Understanding where they overlap or differ has been described fairly well by others (<a href="https://www.datarevenue.com/en-blog/airflow-vs-luigi-vs-argo-vs-mlflow-vs-kubeflow">this one</a>, or <a href="https://medium.com/arthur-engineering/picking-a-kubernetes-orchestrator-airflow-argo-and-prefect-83539ecc69b">that one</a>).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*o7aOVLu1W_ooA2y-YTjW1w.png" /><figcaption>From: <a href="https://star-history.t9t.io/#apache/airflow&amp;argoproj/argo-workflows&amp;spotify/luigi&amp;uber/cadence&amp;PrefectHQ/prefect">star history</a></figcaption></figure><p>This articles focuses on the least overlapping one: <strong>Argo Workflow</strong>.</p><h3>Task orchestration</h3><p>Imagine you are tasked with four items: cleaning data, training a model, evaluating a model, using the model to make inferences on unseen data. In the beginning, this happens ad-hoc, because you’re the only one in the organization performing them. This is fine, as you manage to deliver what the downstream consumer expects from you. It required you almost zero initial effort. Victim of your own success,</p><ul><li>your team grows,</li><li>more similar use-cases are being worked out,</li><li>and more other teams and products will depend on you performing these tasks in a stable manner.</li></ul><p>This is when a task orchestrator comes into play. You leverage this tool to model each task as being a vertex (node) in a graph of tasks. An edge (arrow) represents an execution dependency. This type of graph is called a direct acyclic graph (DAG). You rely on your orchestrator to trigger and monitor these flows reliably.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MeCLUXjk2Z39moK4ISCHAg.png" /><figcaption>Source: author</figcaption></figure><p>Secondly, your orchestrator should be <strong>language or framework agnostic</strong>. You might start-off with a Python specific orchestrator (like <a href="https://github.com/spotify/luigi">Luigi</a>) because it seems easy to get-started. Eventually there will be another “hot thing” that your orchestrator will need to support. Back to the drawing board… Running a task in container on k8s offers this flexibility.</p><p>Thirdly, defining a workflow should be as <strong>light</strong> as possible for end-users. The cognitive load of reading (let alone maintaining) a 250-line DAG-definition is not to be underestimated. Look for easy templating options to remove boilerplate configurations.</p><h3>Using Argo Workflows</h3><p>Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes (k8s). Argo Workflows is implemented as a k8s custom resource definition (CRD). CRD’s are used to define custom API objects. It allows for the extension of the vanilla k8s-experience in a k8s-compliant fashion.</p><p>Argo Workflow is part of the <a href="https://argoproj.github.io/">Argo project</a>, which offers a range of, as they like to call it, Kubernetes-native get-stuff-done tools (Workflow, CD, Events, Rollouts).</p><p>Users can interact with it through the Argo CLI, UI or via kubectl.</p><p>To get a better feel of what the end-user will be dealing with, let’s go over a few key concepts.</p><h4>Core concepts</h4><p>A Workflow is the most fundamental object. It defines and stores the state of a workflow. Consider it as a dynamic object.</p><pre>apiVersion: argoproj.io/v1alpha1 <br>kind: Workflow <br>metadata:<br>  generateName: hello-world-    # Name of this Workflow <br>spec:<br>  entrypoint: whalesay          # Will run &quot;whalesay&quot; step first<br>  templates:   <br>    - name: whalesay            <br>      container:       <br>        image: docker/whalesay       <br>        command: [cowsay]       <br>        args: [&quot;hello world&quot;]</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yIhaN8jbPCOGUavBZZj1FA.png" /><figcaption>Source: author</figcaption></figure><p>Although the basis will always be to run a container, there are other template types, which are divided into 2 groups.</p><p><strong>Template definitions</strong></p><p>These define actual work to be done in a step.</p><ul><li>container — most popular type</li><li>script — templatable convience wrapper for container</li></ul><pre>- name: gen-random-int     <br>  script:       <br>    image: python:alpine3.6       <br>    command: [python]       <br>    source: |         <br>      import random         <br>      i = random.randint(1, 100)         <br>      print(i)</pre><ul><li>resource — any operation on Kubernetes resources</li></ul><p>Example here creates a ConfigMap.</p><pre>- name: k8s-owner-reference     <br>  resource:       <br>    action: create       <br>    manifest: |         <br>      apiVersion: v1         <br>      kind: ConfigMap         <br>      metadata:           <br>        generateName: owned-eg-         <br>      data:           <br>        some: value</pre><ul><li>suspend — more useful than you think</li></ul><pre>- name: delay     <br>  suspend:       <br>    duration: &quot;20s&quot;</pre><p><strong>Template invocations</strong></p><p>These invoke other template types. Typically defines a structure.</p><ul><li>steps — define steps in a “list of lists” way</li></ul><p>Example here runs step1 first, then step2a and step2b in parallel.</p><pre>- name: hello-hello-hello     <br>  steps:     <br>    - - name: step1         <br>        template: prepare-data     <br>    - - name: step2a         <br>        template: run-data-first-half       <br>      - name: step2b         <br>        template: run-data-second-half</pre><ul><li>dag — define steps as a dependency graph.</li></ul><pre>- name: diamond     <br>  dag:       <br>    tasks:       <br>    - name: A         <br>      template: echo       <br>    - name: B         <br>      dependencies: [A]         <br>      template: echo       <br>    - name: C         <br>      dependencies: [A]         <br>      template: echo       <br>    - name: D         <br>      dependencies: [B, C]         <br>      template: echo</pre><p><strong>WorkflowTemplate to the rescue!</strong></p><p>In order to reduce the number of lines of text in Workflow YAML files, use WorkflowTemplate . This allow for re-use of common components. The basic hello-world example becomes. This is similar to the k8s-native podTemplate.</p><pre>apiVersion: argoproj.io/v1alpha1 <br>kind: WorkflowTemplate <br>metadata:<br>  name: workflow-template-submittable<br>spec: <br>  arguments: <br>    parameters: <br>      - name: message <br>        value: hello world <br>  templates: <br>    - name: whalesay-template <br>      inputs: <br>        parameters: <br>          - name: message <br>      container: <br>        image: docker/whalesay <br>        command: [cowsay] <br>        args: [&quot;{{inputs.parameters.message}}&quot;]</pre><p>More concepts and examples can be found in the <a href="https://argoproj.github.io/argo-workflows/">documentation</a>.</p><h3>Cool, but why Argo Workflow and not just Airflow or something else?</h3><p>Argo is designed to run on top of k8s. Not a VM, not AWS ECS, not Container Instances on Azure, not Google Cloud Run or App Engine. This means you get all the good of k8s, but also the bad.</p><p>If you are already quite invested in k8s, then it makes sense to first look at Argo. You will recognise all of the mechanisms known in vanilla k8s.</p><h4>The good</h4><ul><li>Resilience to container crashes and failures, inherited from k8s.</li><li>Autoscaling and options to configure this. Simultaneously triggering 100’s or 1000’s of Argo Workflows is not a problem with minimal tuning (setting cpu and memory requirements per task correctly, etc).</li><li>Possibility for endless configurability.</li><li>Full support for RBAC, inherited from k8s. Their RBAC model also integrates <a href="https://argoproj.github.io/argo-workflows/argo-server-sso/">nicely with SSO</a>. For full isolation requirements (each project has its own k8s namespace and own privileges), common in enterprises, this is a big plus compared to Airflow.</li></ul><h4>The bad</h4><p>The relevance of the following three considerations will depend on your situation at hand.</p><h4>#1. Everyone will write and maintain YAML files</h4><p>A short YAML file for a single project is maintainable. Once the number of workflows start increasing, and the requirements become more complex, Argo offers you tricks and templating features to keep it manageable.</p><p>If you’re organization is used to this way of working (thanks to the use of other k8s-native tools), then you might find it acceptable. Otherwise, don’t jump for it yet.</p><p>Just look at the <a href="https://github.com/argoproj/argo-workflows/tree/master/examples">official examples</a> to get a feel of how your repo will look like.</p><h4>#2. Users will need to be Kubernetes experts</h4><p>If your team consists of seasoned k8s experts, using Argo will feel like second nature. A novice user will first need to understand containers and k8s. That burden might be a huge slow down initially. On the other side, this cost is fair if IT management is betting on the full k8s way of working.</p><p>For maintainers of the Argo setup it is even more important to know your way in k8s. Most probably also be very knowledgeable of AWS, GCP or Azure.</p><h4>#3. Maintenance of a full-fledged enterprise setup is heavy</h4><p>Installing Argo Workflow on an existing k8s cluster is relatively easy.</p><pre>kubectl create ns argo<br>kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/namespace-install.yaml</pre><p>Maintaining all the YAML files to enable for an enterprise-IT-security-compliant setup is not something to be lighthearted about. Have a look at the number of configurations options of the <a href="https://github.com/argoproj/argo-helm">community-maintained Helm-chart</a> to get a feel for the number of moving parts.</p><p>To put this more into perspective, configuring and supporting Airflow (and others) to the highest security compliance levels is equally non-trivial. This could explain the high number of “fully-managed” orchestrators out there. For example, AWS recently released <a href="https://aws.amazon.com/managed-workflows-for-apache-airflow/">Amazon Managed Workflows for Apache Airflow</a>. The industry’s cry has been heard.</p><h3>Conclusion</h3><p>If you are already heavily invested in Kubernetes, then yes look into Argo Workflow (and its brothers and sisters from the parent project).</p><p>The broader and harder question you should ask yourself is: to go full k8s-native or not? Look at your team’s cloud and k8s experience, size, growth targets. Most probably you will land somewhere in the middle first, as there is no free lunch.</p><p><em>Need help with taking this decision? Let’s get in touch via </em><a href="https://www.linkedin.com/company/data-minded"><em>LinkedIn</em></a><em> or our </em><a href="https://dataminded.be"><em>website</em></a><em>.</em></p><h4>Acknowledgements</h4><p><em>Thank you Data Minded for giving the learning opportunities to produce this article.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=54f6067307a8" width="1" height="1" alt=""><hr><p><a href="https://medium.com/datamindedbe/what-to-consider-before-choosing-argo-workflow-54f6067307a8">What to consider before choosing Argo Workflow?</a> was originally published in <a href="https://medium.com/datamindedbe">datamindedbe</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p> -->
      <p class="post-meta"> 8 min read &nbsp; &middot; &nbsp;
        March 9, 2021
      </p>
      <p class="post-tags">
        <a href="2021">
          <i class="fas fa-calendar fa-sm"></i> 2021 </a>

          
          &nbsp; &middot; &nbsp;
            
            <a href="k8s">
              <i class="fas fa-tag fa-sm"></i> k8s</a> &nbsp;
              
            <a href="argo">
              <i class="fas fa-tag fa-sm"></i> argo</a> &nbsp;
              
            <a href="kubernetes">
              <i class="fas fa-tag fa-sm"></i> kubernetes</a> &nbsp;
              
            <a href="orchestration">
              <i class="fas fa-tag fa-sm"></i> orchestration</a> &nbsp;
              
            <a href="yaml">
              <i class="fas fa-tag fa-sm"></i> yaml</a> &nbsp;
              
          
    </p>
    </li>

    
    
    


    

    <li>
      <h3><a class="post-title" href="https://medium.com/datamindedbe/how-to-share-tabular-data-in-a-privacy-preserving-way-c72a59c7602f?source=rss-6c50ab7de1d9------2">How to share tabular data in a privacy-preserving way</a>
      </h3>
      <!-- <p><h4>Adding noise to existing rows, only adding noise to outcomes of tasks performed on that data, or synthetic data generation? An intuition.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*ojbvYLZaerjcpx7e.jpg" /><figcaption>Source: <a href="https://pixabay.com/photos/gull-bird-colony-flock-group-5540454/">Pixabay</a></figcaption></figure><p>As companies grow, or as regulations get more strict, or as senior IT architects get up to speed with the latest trends, the need (or obligation) to mitigate privacy and leakage risks get stronger for data processing entities.</p><p>Data anonymization or data tokenization techniques are widely used in this context, even though they still allow for the divulgence of private information (see <a href="https://mostly.ai/why-synthetic-data/">https://mostly.ai/why-synthetic-data/</a> for an easy explanation on why this is).</p><h3>Synthetic data generation</h3><p>Synthetic data is fundamentally different. The goal is to come up with a data generator that shows the same global statistics as the original data. It should be hard to distinguish for a model or person between the original and generated.</p><p>Let’s illustrate this by generating synthetic data on the <a href="https://archive.ics.uci.edu/ml/datasets/covertype">Covertype</a> dataset using the <a href="https://github.com/sdv-dev/TGAN">TGAN model</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*N_PAz55IM9n3pxP04VaaMw.png" /><figcaption>The Covertype dataset. Source screenshot: Author</figcaption></figure><p>After training the model on this table, I generated 5000 rows and plotted a histogram of the <em>Elevation </em>column of original and generated set. Both lines seem to match visually.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/399/1*I51gVBUiW7hIRrgrH9fRLQ.png" /><figcaption>Histogram of Elevation column of original and generated set. Source: Author</figcaption></figure><p>To check for relations between pairs of columns, the pairplot of all the continuous columns is shown. The form that the blue-green dots (generated) form should visually match that of the red dots (original). This is the case, nice!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HL1vHZpg0W1EqQ6w48LJ9w.png" /><figcaption>Pairplots of continuous columns of original and generated set. Source: Author</figcaption></figure><p>If we now look at mutual information (a.k.a. correlation without sign) between columns, then columns that are correlated to each other should also be correlated in the generated set. Reversely, columns without showing no correlation in the original set, should not be correlated in the generated set. A value close to 0 means no correlation, while a value close to 1 means perfect correlation. Great, this is the case!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/352/1*g7wWiJ4-K_hZHLoVpGYkDA.png" /><figcaption>Mutual information between columns original set. Source: Author</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/352/1*sHtPhlTNiVlnSnE33S5BzA.png" /><figcaption>Mutual information between columns generated set. Source Author</figcaption></figure><p>As a last test, I wanted to train a dimensionality-reduction (<a href="https://github.com/lmcinnes/umap">UMAP</a>) technique on the original set, and project the original points to a 2D space. I feed the same projector with the generated set. The orange X’s (generated) should lie in blue point clouds of the original dataset. This is indeed the case. Neat!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/390/1*ymmh8qeDBJyKCeA4igEfGQ.png" /><figcaption>2D projection of generated and original sample using UMAP trained on original data. Source: Author</figcaption></figure><p>Ok, that was fun playing around with. For more serious cases, there are 2 main approaches:</p><ul><li><strong>Rule-based stochastic data generation: </strong>user specifies sampling distributions and specific rules to sample from. For example: <br>- column A: should have female names, <br>- column B: should be a country in Europe,<br>- column C: should be an integer uniformly sampled between 1 and 100 if the country in column B is “France” else a constant. <br>Good frameworks are <a href="https://github.com/joke2k/faker">Faker</a>, <a href="https://github.com/RealImpactAnalytics/trumania">Trumania</a>.</li><li><strong>Deep generative models: </strong>can be used to learn the statistical distribution the real data has been supposedly sampled from. Once you have a good approximation of this distribution, a synthetic arbitrary-sized dataset can be sampled at will from it. This is what all the cool kids do these days.<br>Initiatives worth looking at are <a href="https://github.com/sdv-dev/SDV">Synthetic data vault</a>, <a href="https://github.com/gretelai/gretel-synthetics">Gretel.AI</a>, <a href="https://mostly.ai/">Mostly.ai</a>, <a href="https://www.mdclone.com/mdclone-platform">MDClone</a>, <a href="https://hazy.com/">Hazy</a>.</li></ul><p>Today, you could already set-up a proof-of-concept using synthetic data to solve one of the following common issues faced in IT-organisations:</p><ul><li><strong>No useful data in development environment<br></strong>Let’s say you are working on a data product case (can be anything), where the interesting data resides in a production environment with very strict access policies. Unfortunately, you only have access to a development environment without interesting data.</li><li><strong>God-like data access privileges of data scientists and engineers<br></strong>Let’s say you are a data scientist, and suddenly the security architect has restricted your much needed privileges on production data. How can you still perform your work up to a satisfying degree of quality in these restrictive conditions?</li><li><strong>Sharing confidential data with an untrusted external partner<br></strong>You are part of company X. Organisation Y would like to showcase their latest and greatest data product ( can be anything). They ask you for a data extract such that they can show it to you.</li></ul><h3>How does synthetic data fit in differential privacy?</h3><p>The main promise of synthetically generating data is that no matter what post-processing is done on it, or third-party information is being linked to it, no one will ever be able to know if a single entity is contained in the original set or not, or obtain properties of it. This promise is part of a larger concept called differential privacy (DP).</p><h4>Global vs local differential privacy</h4><p>When talking about DP, it always comes in two kinds.</p><p>Often you’re only interested in the outcome of a specific task (for example, training a model on unsharable patient data from different hospitals, computing the mean number of people who ever committed a crime, …), you should look at global differentially privacy. In this case, an untrusted user will never see the sensitive data. Instead, he or she tells a trusted curator (with global differential privacy mechanisms), which has access to the sensitive data, what operations should be performed. Only the outcome is shared with the untrusted user. Checkout <a href="https://github.com/OpenMined/PySyft">Pysyft</a> and <a href="https://projects.iq.harvard.edu/opendp">OpenDP</a> if you want more information on tools doing this.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*L7Cu09Kx_SCPLLLNNAvNSg.png" /><figcaption>Traditional global differential privacy. Source: Author</figcaption></figure><p>In contrast, if a dataset needs to be shared with an untrusted party, local differential privacy principles come into play. Traditionally this is done by adding noise to each row of a table or database. The amount of noise to be added depends on</p><ul><li>the required level of privacy (the famous epsilon in DP literature),</li><li>the dataset size (a larger dataset needs less noise to achieve the same privacy level),</li><li>the datatype of a column (quantitative, categorical, ordinal).</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*d9vWIhL_wSGqZBqOFiUijw.png" /><figcaption>Traditional local differential privacy. Source: Author</figcaption></figure><p>In theory, for an equal level of privacy, a global DP mechanism (noise added on result) will show more accurate results than a local mechanism (noise on row-level).</p><p><strong>Synthetic data generation techniques could thus be seen as a form of local DP.</strong></p><p>For more in-depth information in these topics, I advice looking at:</p><ul><li><a href="https://www.udacity.com/course/secure-and-private-ai--ud185">https://www.udacity.com/course/secure-and-private-ai--ud185</a>.</li><li><a href="https://medium.com/@arbidha1412/local-and-global-differential-privacy-249aaa3571">https://medium.com/@arbidha1412/local-and-global-differential-privacy-249aaa3571</a></li><li><a href="https://www.openmined.org/">https://www.openmined.org/</a></li></ul><h3>Recommendation</h3><p>Let’s get more concrete. You want to share table containing private information to an untrusted party. Right now, you could either add noise to rows of existing data (local DP), setup and use a trusted system (global DP), or you could generate a synthetic dataset based on the original.</p><h4>Add noise to rows of existing data if</h4><ul><li>you don’t know what operation will be done on the perturbed data once shared,</li><li>you need to periodically share an update of the original data (= have this workflow as part of a stable batch process),</li><li>you and the data owners trust the person/team/organisation that will add the noise to the original data.</li></ul><p>The best starting point are <a href="https://github.com/opendifferentialprivacy/">OpenDP tools</a>.</p><p>The most well known case of differential privacy is the US Census data (see <a href="https://databricks.com/session_na20/using-apache-spark-and-differential-privacy-for-protecting-the-privacy-of-the-2020-census-respondents">https://databricks.com/session_na20/using-apache-spark-and-differential-privacy-for-protecting-the-privacy-of-the-2020-census-respondents</a>). This data gets recomputed and released every three years. It’s mostly numerical data that gets aggregated and published on several levels (county, state, nation-wide).</p><h4><strong>Setup and use a trusted system if</strong></h4><ul><li>the system you have in mind supports the tasks and operation that will be performed on it,</li><li>the underlying data lies in different places and it can’t leave it (for example different hospitals),</li><li>you and the data owners actually trust the actual system and the person/team/organisation setting it up.</li></ul><p>As a user of the sensitive data, you will get more accurate results compared to first approach.</p><p>Many of the frameworks don’t have all the required features yet to get this beast deployed in a secure, scalable, audit-able way. There is yet a lot of engineering involved. But as adoption grows over time, this might become a good alternative for large organisations and consortiums.</p><p>The best starting point for this option is <a href="https://www.openmined.org/">OpenMined</a>.</p><h4>Generate synthetic data if</h4><ul><li>the original table is relatively small (&lt;1M rows, &lt;100 columns),</li><li>ad-hoc generation is enough (no periodic re-generation needed),</li><li>you and the data owners trust the person/team/organisation that will generate synthetic data for you.</li></ul><p>As with the small experiment above, results are promising. It also doesn’t require much knowledge of DP systems in the first place. You could get started today if needed, let it train overnight, and have a sharable synthetic set by tomorrow morning, so to speak.</p><p>The biggest downside is that these complex models can get expensive to train and maintain if the size of the dataset increases. Each table also requires its own full model training (transfer learning not really a thing tabular data). This won’t scale to 100’s of tables, even with a substantial computing resource budget.</p><h4>Otherwise, you’re out of luck.</h4><h3>Conclusion</h3><p>With privacy more important than ever, there are great techniques available today to either generate synthetic data or add noise to existing data. However, they all still have their limitations. Besides a few niche cases, there is no enterprise-grade scalable and flexible tool yet which allows you to share data containing private information to untrusted parties.</p><p>Lastly, data owners still have to trust the methods or systems set in place, which requires a non-trivial leap of faith. This is the biggest challenge.</p><p>For now, if you want to have a go at it (a proof-of-concept, just playing around), checkout any of the links mentioned above.</p><h3>Acknowledgements</h3><p>Thank you Kris Peeters and Gergely Soti of <a href="https://www.dataminded.be/">Data Minded</a> for your feedback on the article.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c72a59c7602f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/datamindedbe/how-to-share-tabular-data-in-a-privacy-preserving-way-c72a59c7602f">How to share tabular data in a privacy-preserving way</a> was originally published in <a href="https://medium.com/datamindedbe">datamindedbe</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p> -->
      <p class="post-meta"> 9 min read &nbsp; &middot; &nbsp;
        November 18, 2020
      </p>
      <p class="post-tags">
        <a href="2020">
          <i class="fas fa-calendar fa-sm"></i> 2020 </a>

          
          &nbsp; &middot; &nbsp;
            
            <a href="generative-adversarial">
              <i class="fas fa-tag fa-sm"></i> generative-adversarial</a> &nbsp;
              
            <a href="differential-privacy">
              <i class="fas fa-tag fa-sm"></i> differential-privacy</a> &nbsp;
              
            <a href="privacy">
              <i class="fas fa-tag fa-sm"></i> privacy</a> &nbsp;
              
            <a href="synthetic-data">
              <i class="fas fa-tag fa-sm"></i> synthetic-data</a> &nbsp;
              
            <a href="openmined">
              <i class="fas fa-tag fa-sm"></i> openmined</a> &nbsp;
              
          
    </p>
    </li>

    
    
    


    

    <li>
      <h3><a class="post-title" href="https://towardsdatascience.com/which-cloud-servicer-provider-ml-platform-do-you-need-69ff5d96b7db?source=rss-6c50ab7de1d9------2">Which cloud servicer provider ML platform do you need?</a>
      </h3>
      <!-- <p><h4><a href="https://towardsdatascience.com/tagged/getting-started">Getting Started</a></h4><h3>Which cloud service provider ML platform do you need?</h3><h4>AWS Sagemaker, Azure ML platform or GCP AI platform? It actually doesn’t matter. Not for industrialisation.</h4><p>First, I’m going to assume that you have chosen a cloud service provider (CSP), or in the position to choose one for your organisation. Secondly, I’m also assuming that you need to be able to build, train, tune, evaluate and deploy a machine learning model, then the first thing you are most likely to do is check out the ML platform of your CSP of choice. Or should you look at all those third-party vendors? How to compare?</p><p>Let’s look at what actually matters, namely, the bigger picture.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*y7AGqoVfd1DK2G_n" /><figcaption>Cartoon adapted from <a href="https://www.cartoonstock.com/cartoonview.asp?catref=CC123672">https://www.cartoonstock.com/cartoonview.asp?catref=CC123672</a>, and edited with permission of author.</figcaption></figure><h3>Experimentation vs Industrialisation</h3><p>In each ML or even data science project, there are two phases with a fundamentally different goal. During experimentation, the goal is to find a model that answers a (business) question as fast as possible. Whereas during industrialisation the goal is to run reliably and automatically.</p><p>Let’s assume an organisation prioritises operational excellence first over business value. Maybe after 2 years, they have a model in production that brings value, but runs like a charm. Most (innovation) projects are killed well before, because no demonstrated value has been shown.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IAGY3wfMBif2glOqC3z9kw.png" /><figcaption>First prove experimentation, then industrialisation. (Image by author)</figcaption></figure><p>So a trajectory that makes sense is one that you see in green in the figure above. Go for the low-hanging fruit first using very basic services that are maybe not suited for production use, but at least value has been shown. It could be that the first 4 project in that space are clear fails, but the fifth one is a huge hit.</p><h4>Typical challenges</h4><p>ML platforms like to boast about how they offer solutions to <em>typical</em> issues related to not using such tools. I think they indeed do. Although, the size of the group of challenges that can not be solved by them is remains large.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kgx3MZ1ZSQfxPytXRC9NQw.png" /><figcaption>Typical challenges of <strong>experimentation</strong> depending on organisation size. (Image by author)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6wunwtjTLLdn8gLDbNyOjA.png" /><figcaption>Typical challenges of <strong>industrialisation </strong>depending on organisation size. (Image by author)</figcaption></figure><h3>Hidden technical debt and MLOps</h3><p>When talking a bit more in-depth about industrialisation of ML projects, there are two terms that you should keep in mind.</p><p>The first one term relates to a NeurIPS-paper that Google published some time ago about the <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">hidden technical debt in machine learning systems</a>. The building of a ML model is only a fraction of the total effort to put a model into production. To get it all right is a complex process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2SxlV-DhCgNCjoFs" /><figcaption>Adapted from <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf</a></figcaption></figure><p>Google also published an article on how <a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">MLOps should look like</a>. They define three levels of maturity. This should leed to more reliability and observability in production.</p><ol><li><strong>MLOps level 0</strong> <strong>🐣: </strong>Manual building and deploying of models.</li><li><strong>MLOps level 1</strong> <strong>🤓</strong>: Deploy pipelines instead of models.</li><li><strong>MLOps level 2</strong> <strong>😎</strong>: CICD integration, automated retraining, concept drift detection.</li></ol><p>By looking at their schematic overview (see figure below) of such a process, you immediately see that it can get complex. Organisations need a solid strategy to get to that level of maturity. For this to succeed in the long run, decision makers need to see value.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zeR6vH-pGch7H6Dx" /><figcaption>Adapted from <a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</a></figcaption></figure><h3>Start small, focus on experimentation</h3><p>We propose the following adoption evolution. It is one that makes sense.</p><p>If you’re organisation has no experience with cloud, ML or Python, start with:</p><ul><li>Jupyter notebooks (or<a href="https://medium.com/@mohtedibf/the-evolution-of-jupyter-notebook-jupyter-lab-704f3e93230c"> Jupyter labs</a>),</li><li><a href="https://medium.com/analytics-vidhya/a-critical-overview-of-automl-solutions-cb37ab0eb59e">AutoML</a>,</li><li><a href="https://towardsdatascience.com/drag-and-drop-tools-for-machine-learning-pipelines-worth-a-try-63ace4a18715">drag&amp;drop tools</a>.</li></ul><p>They are basic, and for experimentation, yet can show business quick value. This allows for more buy-in and confidence from business towards IT to look at features that will increase stability, traceability of your industrialised models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pgtPpF5ylKBmL_Ebxy6P6w.png" /><figcaption>(Image by author)</figcaption></figure><p>The first feature you want to look at once you’re ready to more towards industrialisation, is a model registry. It’s a place where trained models are tracked and can be used for deployment. It can help you track which one performed well, or didn’t. Also, if you want to see how exactly that <em>one </em>good model from 2 months ago was implemented, then this is of great help.</p><p>Secondly, you often don’t want to only train and deploy a model. A typical pipeline consists of</p><ol><li>a data-preprocessing,</li><li>model tuning,</li><li>model training,</li><li>model evaluation (with harder-to-compute metrics),</li><li>final model training on all data,</li><li>model deployment step.</li></ol><p>From this point on, you don’t want to play around anymore in notebooks, but create a Python modules that are tracked through for example Git.</p><p>Thirdly, until now I assume you only did batch inference (optimised for throughput). For example, run a batch pipeline every morning at 2am that will perform prediction on a set of data collected from the previous day. There will be a moment, where you also want to be able to do it <em>all the time</em>, in a so called online fashion (a.k.a. request-reply). To materialise this ability, an API endpoint can be employed. It returns a prediction from a deployed model on every message it receives. This flow is then optimised for latency. You minimise the time it takes for a message sender to receive a valid response from an API endpoint.</p><p>Finally, integrate your ML pipelines in a CICD flow. Set-up triggers for retraining. This is the last step. Depending on the organisations experience with setting up flows for normal IT systems, the adoption speed may vary. Setting-up such a flow can be seen as going through hell. You want only a few people to go through hell. Push them to produce compelling template for other less-experienced data scientists or engineers to use.</p><h3>Cool, but what ML platform should I choose?</h3><p>The answer is: it depends.</p><p>GCP AI platform, Azure ML and AWS Sagemaker have a lot of common services. They all provide the functionality shown in the summary below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hEqdZkSc2IzRiagBj5TVfQ.png" /><figcaption>(Image by author)</figcaption></figure><p>The differences between them are notable. It should not be a dealbreaker to not go with a specific platform. Even though it was hard for me to find clear 👍’s or 👎’s, I summarise very few clear ones below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dxgJyvitZDEphDzDJhauqA.png" /><figcaption>Disclaimer, this is my personal opinion based on having used the 3 different platform. (Image by author)</figcaption></figure><h3>Should I change ML platform?</h3><p>If you’re a startup and want an ML platform, and you still have the luxury to choose a cloud provider, start from your needs. Do your research, and base your choice on those. You don’t need an expensive-to-manage Kubeflow platform on top of a Kubernetes cluster for deploying a few models.</p><p>If you’re a larger organisation, and already made a choice for a cloud provider, we advice to stay with it. Don’t change for a few extra bells and whistles. Like social media platforms, in the end they will all look and do the same. Google might have a better AutoML tool, but I’m pretty sure that they will Azure and AWS will catch-up (if they haven’t already).</p><p>Note that deploying more than 5–10 models can start to get expensive on any of these platforms.</p><h3>General recommendation</h3><h4>1. For <strong>experimentation</strong></h4><p>Focus on fast iterations ⚡, self-serviceness 💁 and easy debugging 🐛.</p><p>Pick any tool where you are the most productive with. AWS Sagemaker, GCP AI platform or Azure ML definitely help with speeding-up</p><ul><li>exploration,</li><li>model tuning,</li><li>training,</li><li>setting a benchmark.</li></ul><h4>2. For <strong>Industrialisation</strong></h4><p>Focus on reliability 🧘‍♂️, traceability 🔍 and avoiding technology fragmentation 🧩.</p><p>If you’re a large organisation, look for integrating model industrialisation with existing stable data engineering chain in the cloud (on Kubernetes for example). <br>If you’re a start-up, use the deployment options of these ML platforms. This is a clear accelerator for you. Certainly if the number of deployed models is subjectively small (&lt;5–10 models). If you notice after a while that</p><ul><li>costs are high,</li><li>data access problems or</li><li>instabilities persist,</li></ul><p>then it’s time to move away from those ML platforms. Move towards a stable data engineering chain (as with large companies).</p><p>Finally, you can also check out our webinar exactly on this topic.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FkjhXMTOLtac%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DkjhXMTOLtac&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkjhXMTOLtac%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/57e604963579258a3081fa20275b4ba1/href">https://medium.com/media/57e604963579258a3081fa20275b4ba1/href</a></iframe><h3>Bonus: Tips to smooth transition to go from experimentation to industrialisation</h3><p><strong>Decouple the science and the engineering code<br></strong>Machine learning often has a lot boilerplate code that needs to be added. Think of training loops, data loading, logging,… If you use Pytorch, definitely have a look at <a href="https://github.com/PyTorchLightning/pytorch-lightning">Pytorch Lightning</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*av-DDR67gsYQ0v67qapaQg.gif" /><figcaption>From: <a href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/docs/source/_images/general/fast_2.gif">https://github.com/PyTorchLightning/pytorch-lightning/blob/master/docs/source/_images/general/fast_2.gif</a></figcaption></figure><h4><strong>Adopt project code templates</strong></h4><p>Every project inside your organisation will need some code to</p><ul><li>integrate with a CICD system,</li><li>integrate with the chosen ML platform,</li><li>setup data access,</li><li>setup logging, etc.</li></ul><p>Use something called a <a href="https://github.com/cookiecutter/cookiecutter">cookiecutter</a>, to generate project structure in a git repo, that will unburden a new project from all this hassle. Have a look at <a href="https://github.com/drivendata/cookiecutter-data-science">https://github.com/drivendata/cookiecutter-data-science</a> for a good starting point.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gepnXqEyrwwgxF69e_lt1Q.png" /><figcaption>From <a href="https://github.com/drivendata/cookiecutter-data-science">https://github.com/drivendata/cookiecutter-data-science</a></figcaption></figure><h4><strong>Standardize</strong></h4><p>Limit the number of technology stacks used, simply put. The more you use, the more you’ll have to maintain. This also eases out the transition when a data science leaves his or her model to a data engineer.</p><h3>Acknowledgements</h3><p>I would like to thank Kristof Martens, Gergely Soti, Pascal Knapen and mostly Kris Peeters from <a href="https://www.dataminded.be/">Data Minded</a> for their input and feedback. But also our clients which enable to work with so many different technology stacks.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=69ff5d96b7db" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/which-cloud-servicer-provider-ml-platform-do-you-need-69ff5d96b7db">Which cloud servicer provider ML platform do you need?</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p> -->
      <p class="post-meta"> 9 min read &nbsp; &middot; &nbsp;
        October 13, 2020
      </p>
      <p class="post-tags">
        <a href="2020">
          <i class="fas fa-calendar fa-sm"></i> 2020 </a>

          
          &nbsp; &middot; &nbsp;
            
            <a href="azure-ml">
              <i class="fas fa-tag fa-sm"></i> azure-ml</a> &nbsp;
              
            <a href="machine-learning">
              <i class="fas fa-tag fa-sm"></i> machine-learning</a> &nbsp;
              
            <a href="getting-started">
              <i class="fas fa-tag fa-sm"></i> getting-started</a> &nbsp;
              
            <a href="sagemaker">
              <i class="fas fa-tag fa-sm"></i> sagemaker</a> &nbsp;
              
            <a href="ai-platform">
              <i class="fas fa-tag fa-sm"></i> ai-platform</a> &nbsp;
              
          
    </p>
    </li>

    
  </ul>

  <nav aria-label="Blog page naviation">
  <ul class="pagination pagination-lg justify-content-center">
    <li class="page-item ">
      <a class="page-link" href="/blog/" tabindex="-1" aria-disabled="1">Newer</a>
    </li><li class="page-item "><a class="page-link" href="/blog/index.html" title="blog">1</a></li>
      <li class="page-item active"><a class="page-link" href="/blog/page/2/index.html" title="blog - page 2">2</a></li>
      <li class="page-item disabled">
      <a class="page-link" href="">Older</a>
    </li>
  </ul>
</nav>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2022 Bruno  Coussement. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.
Last updated: January 27, 2022.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

